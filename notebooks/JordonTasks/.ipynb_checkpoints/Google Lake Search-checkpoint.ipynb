{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f70bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 429: Too Many Requests",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m lake \u001b[38;5;241m=\u001b[39m lake\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/customsearch/v1?key=AIzaSyBZl683A8IrcIiwPLT4gchHo2DdqcgujFs&cx=643b7acb0336d498a&q=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mlake\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+AND+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlgal+bloom\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&alt=json&fields=queries(request(totalResults))\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 16\u001b[0m html \u001b[38;5;241m=\u001b[39m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     17\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m site_json\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mloads(soup\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/envs/Satpy/lib/python3.8/urllib/request.py:222\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Satpy/lib/python3.8/urllib/request.py:531\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    530\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 531\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/Satpy/lib/python3.8/urllib/request.py:640\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 640\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/Satpy/lib/python3.8/urllib/request.py:569\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    568\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Satpy/lib/python3.8/urllib/request.py:502\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    501\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/Satpy/lib/python3.8/urllib/request.py:649\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 429: Too Many Requests"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "lakeinput = pd.read_csv('/home/hboi-ouri/Projects/NASA_ProjectExp/outputs/JBTask/Google Lake Search Task/LakeList.csv')\n",
    "\n",
    "lakelist = lakeinput['Lake']\n",
    "\n",
    "resultnumber = []\n",
    "\n",
    "for lake in lakelist:\n",
    "    lake = lake.replace(' ','+')\n",
    "    url = 'https://www.googleapis.com/customsearch/v1?key=AIzaSyBZl683A8IrcIiwPLT4gchHo2DdqcgujFs&cx=643b7acb0336d498a&q=\"'+lake+'\"+AND+\"Algal+bloom\"&alt=json&fields=queries(request(totalResults))'\n",
    "    html = request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    site_json=json.loads(soup.text)\n",
    "\n",
    "#     print([d.get('totalResults') for d in site_json['queries']['request'] if d.get('totalResults')])\n",
    "    \n",
    "    resultnumber.append([d.get('totalResults') for d in site_json['queries']['request'] if d.get('totalResults')])\n",
    "\n",
    "csv_input = pd.read_csv('/home/hboi-ouri/Projects/NASA_ProjectExp/outputs/JBTask/Google Lake Search Task/LakeList.csv')\n",
    "csv_input['Number of Results'] = resultnumber\n",
    "print(resultnumber)\n",
    "# csv_input.to_csv('/home/hboi-ouri/Projects/NASA_ProjectExp/outputs/JBTask/Google Lake Search Task/LakeList.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "268b1542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index']\n",
      "['1']\n",
      "['2']\n",
      "['3']\n",
      "['4']\n",
      "['5']\n",
      "['6']\n",
      "['7']\n",
      "['8']\n",
      "['9']\n",
      "['10']\n",
      "['11']\n",
      "['12']\n",
      "['13']\n",
      "['14']\n",
      "['15']\n",
      "['16']\n",
      "['17']\n",
      "['18']\n",
      "['19']\n",
      "['20']\n",
      "['21']\n",
      "['22']\n",
      "['23']\n",
      "['24']\n",
      "['25']\n",
      "['26']\n",
      "['27']\n",
      "['28']\n",
      "['29']\n",
      "['30']\n",
      "['31']\n",
      "['32']\n",
      "['33']\n",
      "['34']\n",
      "['35']\n",
      "['36']\n",
      "['37']\n",
      "['38']\n",
      "['39']\n",
      "['40']\n",
      "['41']\n",
      "['42']\n",
      "['43']\n",
      "['44']\n",
      "['45']\n",
      "['46']\n",
      "['47']\n",
      "['48']\n",
      "['49']\n",
      "['50']\n",
      "['51']\n",
      "['52']\n",
      "['53']\n",
      "['54']\n",
      "['55']\n",
      "['56']\n",
      "['57']\n",
      "['58']\n",
      "['59']\n",
      "['60']\n",
      "['61']\n",
      "['62']\n",
      "['63']\n",
      "['64']\n",
      "['65']\n",
      "['66']\n",
      "['67']\n",
      "['68']\n",
      "['69']\n",
      "['70']\n",
      "['71']\n",
      "['72']\n",
      "['73']\n",
      "['74']\n",
      "['75']\n",
      "['76']\n",
      "['77']\n",
      "['78']\n",
      "['79']\n",
      "['80']\n",
      "['81']\n",
      "['82']\n",
      "['83']\n",
      "['84']\n",
      "['85']\n",
      "['86']\n",
      "['87']\n",
      "['88']\n",
      "['89']\n",
      "['90']\n",
      "['91']\n",
      "['92']\n",
      "['93']\n",
      "['94']\n",
      "['95']\n",
      "['96']\n",
      "['97']\n",
      "['98']\n",
      "['99']\n",
      "['100']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "        \n",
    "lakeinput = pd.read_csv('/home/hboi-ouri/Documents/LakeList.csv')\n",
    "\n",
    "indexlist = lakeinput['Index']\n",
    "lakelist = lakeinput['Lake']\n",
    "    \n",
    "#################\n",
    "\n",
    "resultnumber = []\n",
    "\n",
    "for lake in lakelist:\n",
    "    lake = lake.replace(' ','+')\n",
    "    url = 'https://www.googleapis.com/customsearch/v1?key=AIzaSyBZl683A8IrcIiwPLT4gchHo2DdqcgujFs&cx=643b7acb0336d498a&q=\"'+lake+'\"+AND+\"Algal+bloom\"&alt=json&fields=queries(request(totalResults))'\n",
    "    html = request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    site_json=json.loads(soup.text)\n",
    "    \n",
    "    resultnumber.append([d.get('totalResults') for d in site_json['queries']['request'] if d.get('totalResults')])\n",
    "    \n",
    "###################\n",
    "\n",
    "field_names = ['Index','Lake','Number of Results']\n",
    "  \n",
    "\n",
    "i=0\n",
    "while i<=100:\n",
    "    dict = {\"Index\": indexlist[i], \"Lake\": lakelist[i], \"Number of Results\": resultnumber[i]}\n",
    "    \n",
    "    with open('/home/hboi-ouri/Documents/LakeList.csv', 'a') as csv_file:\n",
    "    dict_object = csv.DictWriter(csv_file, fieldnames=field_names) \n",
    "    dict_object.writerow(dict)\n",
    "    \n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98c8487b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2934548551.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [41]\u001b[0;36m\u001b[0m\n\u001b[0;31m    dict = {\"Index\": item[0], \"Lake\": item[1],\"Number of Results\": }\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f95cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_csv.writer' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m spamwriter \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m, quoting\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mQUOTE_MINIMAL)\n\u001b[1;32m      5\u001b[0m spamwriter\u001b[38;5;241m.\u001b[39mwriterow([lakeinput[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLake\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m \u001b[43mspamwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/hboi-ouri/Projects/NASA_ProjectExp/outputs/JBTask/Google Lake Search Task/LakeList.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_csv.writer' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "  \n",
    "# Dictionary\n",
    "dict = {\"Index\": , \"Lake\": ,\"Number of Results\": ,}\n",
    "\n",
    "with open('demo_csv.csv', 'a') as csv_file:\n",
    "    dict_object = csv.DictWriter(csv_file, fieldnames=field_names) \n",
    "  \n",
    "    dict_object.writerow(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88e072c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lake Superior', 'Lake Huron', 'Lake Michigan', 'Lake Erie', 'Lake Ontario', 'Lake of the Woods', 'Iliamna Lake', 'Great Salt Lake', 'Lake Oahe', 'Lake Okeechobee', 'Lake Pontchartrain', 'Lake Sakakawea', 'Lake Champlain', 'Becharof Lake', 'Lake St. Clair', 'Red Lake', 'Selawik Lake', 'Fort Peck Lake', 'Salton Sea', 'Rainy Lake', 'Teshekpuk Lake', 'Devils Lake', 'Toledo Bend Reservoir', 'Lake Powell', 'Kentucky Lake', 'Lake Mead', 'Naknek Lake', 'Lake Winnebago', 'Mille Lacs Lake', 'Flathead Lake', 'Lake Tahoe', 'Pyramid Lake', 'Sam Rayburn Reservoir', 'Eufaula Lake', 'Lake Marion', 'Leech Lake', 'Utah Lake', 'Lake Francis Case', 'Lake Pend Oreille', 'Lake Texoma', 'Yellowstone Lake', 'Falcon Lake', 'Lake Livingston', 'Franklin D. Roosevelt Lake', 'Lake Clark', 'Moosehead Lake', 'Lake Strom Thurmond', 'Bear Lake', 'Lake Guntersville', 'Lake St. Francis', 'Wheeler Lake', 'Amistad Lake', 'Klamath Lake', 'Tustumena Lake', 'Lake Moultrie', 'Lake Winnibigoshish', 'Lake Barkley', 'Lake Sharpe', 'American Falls Reservoir', 'Lake Hartwell', 'Truman Reservoir', 'Lake of the Ozarks', 'Oneida Lake', 'Lake Cumberland', 'Kerr Lake', 'Calcasieu Lake', 'Lake Murray', 'Grand Lake oâ€™ the Cherokees', 'Lake Koocanusa', 'Lake George', 'Lake Winnipesaukee', 'Bull Shoals Lake', 'Walter F. George Lake', 'Lake Martin', 'Clear Lake', 'Robert S. Kerr Lake', 'Seneca Lake', 'Pickwick Lake', 'Table Rock Lake', 'Cayuga Lake', 'Flaming Gorge Reservoir', 'Richland-Chambers Reservoir', 'Lake Vermilion', 'Lake Ouachita', 'Lake Mattamuskeet', 'Watts Bar Lake', 'Lake Wallula ', 'Lake Lanier', 'Lake Tawakoni', 'Elephant Butte Lake', 'Chickamauga Lake', 'Grenada Lake', 'Lake Kissimmee', 'Lake Dardanelle', 'Norris Lake', 'Canyon Ferry Lake', 'Lake Chelan', 'Cedar Creek Lake', 'Lake Norman', 'Sardis Lake']\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "lakeinput = pd.read_csv('/home/hboi-ouri/Projects/NASA_ProjectExp/outputs/JBTask/Google Lake Search Task/LakeList.csv')\n",
    "\n",
    "lakelist = lakeinput['Lake']\n",
    "indexlist = lakeinput['Index']\n",
    "fulllakelist = zip(lakelist, indexlist)\n",
    "print(list(fulllakelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b976209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2708ad9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
